#!/usr/bin/env python3
# GlobalStockNow AI Analyzer v0.2 (2026.1.2)
# Qwen2.5-1.5B-Instructë¡œ í•´ì™¸ ì†ë³´ â†’ í•œêµ­ ì‹œì¥ ì˜í–¥ ë¶„ì„
# ë‚˜ì¤‘ì— ê³ í’ˆì§ˆ ì›í•˜ì‹œë©´ model_nameë§Œ "Qwen/Qwen2.5-7B-Instruct"ë¡œ ë°”ê¾¸ì„¸ìš”

import json
from datetime import datetime
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# ë¶„ì„ëœ ë‰´ìŠ¤ ì €ì¥ ë¦¬ìŠ¤íŠ¸
analyzed_news = []

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ì‚¬ì—…ê³„íšì„œ í˜ì´ì§€ 9 ê¸°ë°˜)
PROMPT_TEMPLATE = """
ë„ˆëŠ” í•œêµ­ ì£¼ì‹ ì „ë¬¸ê°€ì´ì ê¸€ë¡œë²Œ ê²½ì œ ë¶„ì„ê°€ë‹¤.
ì•„ë˜ í•´ì™¸ ì†ë³´ë¥¼ ë¶„ì„í•´ì„œ í•œêµ­ ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ 10ì  ë§Œì ìœ¼ë¡œ í‰ê°€í•´ì¤˜.

ë‰´ìŠ¤ ì œëª©: {title}
ë‰´ìŠ¤ ìš”ì•½: {summary}
ë°œí–‰ ì‹œê°„: {published}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•´ (ì„¤ëª… ì—†ì´ JSONë§Œ ì¶œë ¥):
{
  "impact_score": 0.0 ~ 10.0 (ìˆ«ì, í•œêµ­ ì‹œì¥ ì˜í–¥ë„. 7.0ì  ë¯¸ë§Œì€ ë¶„ì„ ì œì™¸),
  "impact_period": "ë‹¨ê¸°(1~3ì¼)" ë˜ëŠ” "ì¤‘ê¸°(1~4ì£¼)" ë˜ëŠ” "ì¥ê¸°(1ê°œì›” ì´ìƒ)",
  "related_korean_stocks": ["ì¢…ëª©ëª…1", "ì¢…ëª©ëª…2", ...] (ìµœëŒ€ 3ê°œ, í•œêµ­ ìƒì¥ì‚¬ë§Œ),
  "key_points": "í•œêµ­ íˆ¬ììì—ê²Œ ì¤‘ìš”í•œ ì‹œì‚¬ì  í•œ ë¬¸ì¥ (30ì ì´ë‚´)"
}

ë¶„ì„ ì‹œì‘:
"""

print("ğŸ§  GlobalStockNow AI ë¶„ì„ ì‹œì‘! Qwen2.5 ëª¨ë¸ ë¡œë”© ì¤‘...")

# ëª¨ë¸ ì„ íƒ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš© 1.5B, ê³ í’ˆì§ˆ ì›í•  ë•Œ 7Bë¡œ ë³€ê²½)
model_name = "Qwen/Qwen2.5-1.5B-Instruct"   # â† ì—¬ê¸°ë§Œ ë°”ê¾¸ë©´ ë¨

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_4bit=True   # ë©”ëª¨ë¦¬ ì ˆì•½ (CPUì—ì„œë„ ê°€ëŠ¥)
)

print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! ë¶„ì„ ì‹œì‘")

# breaking_news.json ì½ê¸°
try:
    with open('breaking_news.json', 'r', encoding='utf-8') as f:
        news_list = json.load(f)
    print(f"ğŸ“¥ {len(news_list)}ê°œ ì†ë³´ ë¡œë“œ ì™„ë£Œ")
except FileNotFoundError:
    print("âŒ breaking_news.json íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. collector.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    exit()
except Exception as e:
    print(f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
    exit()

# ê°œë³„ ë‰´ìŠ¤ ë¶„ì„
for idx, item in enumerate(news_list, 1):
    print(f"\n[{idx}/{len(news_list)}] ë¶„ì„ ì¤‘: {item['title'][:70]}...")
    
    prompt = PROMPT_TEMPLATE.format(
        title=item['title'],
        summary=item.get('summary', 'ìš”ì•½ ì—†ìŒ')[:500],
        published=item['published']
    )
    
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=200,
            temperature=0.3,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œê±°
    response = response[len(prompt):].strip()
    
    try:
        # JSON íŒŒì‹± (ë•Œë•Œë¡œ ëª¨ë¸ì´ ```json ë¸”ë¡ìœ¼ë¡œ ê°ìŒ€ ìˆ˜ ìˆìŒ)
        if response.startswith("```json"):
            response = response[7:]
        if response.endswith("```"):
            response = response[:-3]
        result = json.loads(response)
        
        score = float(result.get("impact_score", 0))
        if score >= 7.0:
            result["original_title"] = item["title"]
            result["original_link"] = item["link"]
            result["analyzed_at"] = datetime.utcnow().strftime('%Y-%m-%d %H:%M UTC')
            analyzed_news.append(result)
            print(f"   âœ… ì˜í–¥ë„ {score:.1f}ì  â†’ ì €ì¥")
        else:
            print(f"   â© ì˜í–¥ë„ {score:.1f}ì  â†’ ìŠ¤í‚µ (7ì  ë¯¸ë§Œ)")
    except json.JSONDecodeError:
        print("   âŒ JSON íŒŒì‹± ì‹¤íŒ¨ â†’ ìŠ¤í‚µ")
    except Exception as e:
        print(f"   âŒ ë¶„ì„ ì˜¤ë¥˜: {e} â†’ ìŠ¤í‚µ")

# ìµœì¢… ê²°ê³¼ ì €ì¥
final_count = len(analyzed_news)
print(f"\nğŸ¯ ìµœì¢… ë¶„ì„ ì™„ë£Œ: {final_count}ê°œ ê³ ì˜í–¥ ì†ë³´ ì„ ë³„!")

with open('analyzed_news.json', 'w', encoding='utf-8') as f:
    json.dump(analyzed_news, f, indent=2, ensure_ascii=False)

print("ğŸ’¾ analyzed_news.json ì €ì¥ ì™„ë£Œ â†’ ë‹¤ìŒ ë‹¨ê³„(ì½˜í…ì¸  ìƒì„±) ì¤€ë¹„ OK!")
