#!/usr/bin/env python3
# GlobalStockNow News Collector v0.1 (2026.1.3)
# Google News RSSë¡œ í•´ì™¸ ì£¼ì‹ ì†ë³´ ìˆ˜ì§‘

import feedparser
from datetime import datetime, timedelta
import json

# í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (30ê°œ ì´ìƒ)
STOCK_KEYWORDS = [
    'nvidia', 'amd', 'intel', 'tsmc', 'samsung', 'sk hynix', 'semiconductor', 'chip',
    'fed', 'federal reserve', 'interest rate', 'powell',
    'tesla', 'ev', 'battery', 'byd',
    'apple', 'iphone', 'aapl', 'google', 'msft', 'amazon', 'meta',
    'oil', 'opec', 'energy',
    'china', 'trade war', 'tariff',
    'bitcoin', 'crypto', 'ethereum'
]

def collect_breaking_news(max_hours=6, max_items=20):
    print(f"ðŸš€ GlobalStockNow ì†ë³´ ìˆ˜ì§‘ ì‹œìž‘ (ìµœê·¼ {max_hours}ì‹œê°„, ìµœëŒ€ {max_items}ê°œ)")

    rss_feeds = [
        'https://news.google.com/rss?hl=en-US&gl=US&ceid=US:en',
        'https://news.google.com/rss/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNRGx6TVdBU0FtVnVHZ0pWVXlnQVAB?hl=en-US&gl=US&ceid=US:en',  # World
        'https://news.google.com/rss/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNREpxYW5BU0FtVnVHZ0pWVXlnQVAB?hl=en-US&gl=US&ceid=US:en',  # Business
        'https://news.google.com/rss/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNRFp1ZEdBU0FtVnVHZ0pWVXlnQVAB?hl=en-US&gl=US&ceid=US:en'   # Technology
    ]

    all_news = []
    cutoff_time = datetime.utcnow() - timedelta(hours=max_hours)

    for feed_url in rss_feeds:
        feed = feedparser.parse(feed_url)
        print(f"ðŸ“¡ {feed_url}ì—ì„œ {len(feed.entries)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")

        for entry in feed.entries:
            if len(all_news) >= max_items * 2:  # ì—¬ìœ  ìžˆê²Œ
                break

            pub_time = entry.get('published_parsed')
            if not pub_time:
                continue
            pub_dt = datetime(*pub_time[:6])
            if pub_dt < cutoff_time:
                continue

            title = entry.title.lower()
            summary = (entry.get('summary', '') or '').lower()
            content = title + ' ' + summary

            matched = [kw for kw in STOCK_KEYWORDS if kw.lower() in content]
            if matched:
                all_news.append({
                    'title': entry.title,
                    'link': entry.link,
                    'published': pub_dt.strftime('%Y-%m-%d %H:%M UTC'),
                    'keywords': matched[:3],
                    'summary': entry.get('summary', '')[:200]
                })

    # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
    seen = set()
    unique_news = []
    for item in all_news:
        if item['title'] not in seen:
            seen.add(item['title'])
            unique_news.append(item)

    final_news = unique_news[:max_items]

    print(f"âœ… ìµœì¢… {len(final_news)}ê°œ ì†ë³´ ì„ ë³„ ì™„ë£Œ!")

    # TOP 5 ì¶œë ¥
    print("\nðŸ“Š ìˆ˜ì§‘ëœ ì£¼ìš” ì†ë³´:")
    for i, item in enumerate(final_news[:5], 1):
        print(f"{i}. {item['title']}")
        print(f"   ë§í¬: {item['link']}")
        print(f"   í‚¤ì›Œë“œ: {', '.join(item['keywords'])}")
        print()

    # JSON ì €ìž¥
    with open('breaking_news.json', 'w', encoding='utf-8') as f:
        json.dump(final_news, f, indent=2, ensure_ascii=False)

    print("ðŸ’¾ breaking_news.json ì €ìž¥ ì™„ë£Œ")

if __name__ == "__main__":
    collect_breaking_news()
