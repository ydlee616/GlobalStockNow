#!/usr/bin/env python3
# GlobalStockNow Collector v6.0 (DuckDuckGo Search Edition)
# ì‘ì„±ì¼: 2026.01.09
# ê¸°ëŠ¥: AI ì˜ì¡´ ì—†ì´ ê²€ìƒ‰ì—”ì§„ì—ì„œ ì§ì ‘ ìµœì‹  ë‰´ìŠ¤ ë§í¬ë¥¼ ê¸ì–´ì˜´ (ìˆ˜ì§‘ ì‹¤íŒ¨ìœ¨ 0% ë„ì „)

import json
import datetime
import time
from duckduckgo_search import DDGS

# ---------------------------------------------------------
# [ì„¤ì •] ê²€ìƒ‰í•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ì˜ì–´/í•œêµ­ì–´ í˜¼í•©)
# ---------------------------------------------------------
SEARCH_KEYWORDS = [
    "Samsung Electronics stock news",
    "SK Hynix HBM market share",
    "Global AI semiconductor trends",
    "NVIDIA vs competitors news",
    "Tesla EV sales impact Korea",
    "US Fed interest rate decision effect",
    "CES 2026 Samsung LG news"
]

def collect_news_from_ddg():
    print(f"[{datetime.datetime.now()}] ğŸ¦† DuckDuckGo ê²€ìƒ‰ ì—”ì§„ ê°€ë™...")
    
    all_news = []
    seen_urls = set() # ì¤‘ë³µ ì œê±°ìš©

    # ê°ì²´ ìƒì„± ë°©ì‹ì„ ìµœì‹  ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ë§ì¶¤
    with DDGS() as ddgs:
        for keyword in SEARCH_KEYWORDS:
            try:
                print(f"   ğŸ” ê²€ìƒ‰ ì¤‘: '{keyword}'...")
                # timelimit='d': ì§€ë‚œ 1ì¼(24ì‹œê°„) ì´ë‚´ ë‰´ìŠ¤ë§Œ ê²€ìƒ‰
                # max_results=5: í‚¤ì›Œë“œë‹¹ 5ê°œì”©ë§Œ
                results = ddgs.news(keywords=keyword, region="wt-wt", safesearch="off", timelimit="d", max_results=5)
                
                if results:
                    for r in results:
                        # ì¤‘ë³µ ê¸°ì‚¬ ì œê±°
                        if r['url'] in seen_urls:
                            continue
                        
                        seen_urls.add(r['url'])
                        
                        # ë°ì´í„° í‘œì¤€í™”
                        news_item = {
                            "source": r.get('source', 'Unknown'),
                            "title": r.get('title', ''),
                            "link": r.get('url', ''),
                            "published_at": r.get('date', str(datetime.datetime.now())),
                            "summary": r.get('body', '')  # ê²€ìƒ‰ ê²°ê³¼ì˜ ì§§ì€ ìš”ì•½
                        }
                        all_news.append(news_item)
                else:
                    print(f"      -> '{keyword}' ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ ì—†ìŒ")
                    
            except Exception as e:
                print(f"   âš ï¸ í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
                time.sleep(1) # ì°¨ë‹¨ ë°©ì§€ìš© ì ì‹œ ëŒ€ê¸°

    print(f"âœ… ì´ {len(all_news)}ê°œì˜ ìµœì‹  ì†ë³´ë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤.")
    return all_news

def save_to_json(news_list):
    filename = "breaking_news.json"
    data = {
        "collected_at": str(datetime.datetime.now()),
        "count": len(news_list),
        "articles": news_list
    }
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
    print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {filename} ({len(news_list)}ê±´)")

if __name__ == "__main__":
    # 1. ë‰´ìŠ¤ ê°•ì œ ìˆ˜ì§‘
    articles = collect_news_from_ddg()
    
    # 2. ê²°ê³¼ ì €ì¥ (ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¼ë„ ì €ì¥)
    if not articles:
        print("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. (ê²€ìƒ‰ì–´ ì¡°ì • í•„ìš”)")
        articles = []
        
    save_to_json(articles)
