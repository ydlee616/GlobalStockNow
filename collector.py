#!/usr/bin/env python3
# GlobalStockNow Collector v6.1 (Anti-Block & Fallback Edition)
import json
import datetime
import time
import random
from duckduckgo_search import DDGS

SEARCH_KEYWORDS = [
    "Samsung Electronics stock news",
    "SK Hynix HBM market share",
    "Global AI semiconductor trends",
    "NVIDIA vs competitors news",
    "Tesla EV sales impact Korea",
    "US Fed interest rate decision effect",
    "CES 2026 Samsung LG news"
]

def collect_news_from_ddg():
    print(f"[{datetime.datetime.now()}] ğŸ¦† DuckDuckGo ê²€ìƒ‰ ì—”ì§„ ê°€ë™ (ì•ˆì „ ëª¨ë“œ)...")
    
    all_news = []
    seen_urls = set()

    with DDGS() as ddgs:
        for keyword in SEARCH_KEYWORDS:
            try:
                print(f"   ğŸ” ê²€ìƒ‰ ì‹œë„: '{keyword}'...")
                
                # 1ì°¨ ì‹œë„: ì§€ë‚œ 24ì‹œê°„(d) ë‰´ìŠ¤ ê²€ìƒ‰
                results = list(ddgs.news(keywords=keyword, region="wt-wt", safesearch="off", timelimit="d", max_results=5))
                
                # 2ì°¨ ì‹œë„: ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì§€ë‚œ 1ì£¼ì¼(w)ë¡œ ë²”ìœ„ í™•ì¥ (Fallback)
                if not results:
                    print(f"      ğŸ‘‰ ì˜¤ëŠ˜ ë‰´ìŠ¤ê°€ ì—†ì–´ 'ì§€ë‚œ ì£¼' ë²”ìœ„ë¡œ í™•ì¥í•©ë‹ˆë‹¤.")
                    time.sleep(2) # ì ì‹œ ëŒ€ê¸°
                    results = list(ddgs.news(keywords=keyword, region="wt-wt", safesearch="off", timelimit="w", max_results=3))

                if results:
                    count = 0
                    for r in results:
                        if r['url'] in seen_urls: continue
                        seen_urls.add(r['url'])
                        
                        news_item = {
                            "source": r.get('source', 'Unknown'),
                            "title": r.get('title', ''),
                            "link": r.get('url', ''),
                            "published_at": r.get('date', str(datetime.datetime.now())),
                            "summary": r.get('body', '')
                        }
                        all_news.append(news_item)
                        count += 1
                    print(f"      âœ… {count}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
                else:
                    print(f"      âŒ í™•ì¥ ê²€ìƒ‰ì—ë„ ê²°ê³¼ ì—†ìŒ")

                # ğŸ”¥ í•µì‹¬: ë´‡ ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ ëœë¤ ëŒ€ê¸° (3~6ì´ˆ)
                wait_time = random.uniform(3, 6)
                print(f"      ğŸ’¤ {wait_time:.1f}ì´ˆ ëŒ€ê¸°...")
                time.sleep(wait_time)

            except Exception as e:
                print(f"   âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
                time.sleep(5)

    print(f"âœ… ì´ {len(all_news)}ê°œì˜ ë‰´ìŠ¤ë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤.")
    return all_news

def save_to_json(news_list):
    filename = "breaking_news.json"
    data = {
        "collected_at": str(datetime.datetime.now()),
        "count": len(news_list),
        "articles": news_list
    }
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
    print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {filename}")

if __name__ == "__main__":
    articles = collect_news_from_ddg()
    save_to_json(articles)
