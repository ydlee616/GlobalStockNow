"""Module: collector.py | Version: 0.2.1 | Engineer: Guido Style"""
import feedparser, json, os, urllib.parse
from datetime import datetime, timedelta, timezone
from dateutil import parser

# [ì „ëµ í‚¤ì›Œë“œ] ë©”íƒ€ ì›ì „, ì—”ë¹„ë””ì•„ ì‹ ê¸°ìˆ , HBM, ì „ë ¥ ì¸í”„ë¼ ì§‘ì¤‘ ìˆ˜ì§‘
KEYWORDS = 'Nvidia Pulsar OR "G-Sync" OR "Meta Nuclear" OR "AI Power" OR "HBM3E" OR "Nuclear Energy Deal"'
Q = urllib.parse.quote(KEYWORDS)
FEEDS = {"GNews": f"https://news.google.com/rss/search?q={Q}&hl=en-US&gl=US&ceid=US:en"}

def collect():
    print("ğŸŒ [Ver 0.2.1] 48h ê¸€ë¡œë²Œ ê¸ˆìœµ-ê¸°ìˆ  ë°¸ë¥˜ì²´ì¸ ë°ì´í„° ìˆ˜ì§‘...")
    all_articles = []
    limit = datetime.now(timezone.utc) - timedelta(hours=48)

    for name, url in FEEDS.items():
        feed = feedparser.parse(url)
        for entry in feed.entries[:30]: # ìˆ˜ì§‘ëŸ‰ì„ ëŠ˜ë ¤ ë” ë§ì€ í›„ë³´êµ° í™•ë³´
            try:
                pub_date = parser.parse(entry.get('published', ''))
                if pub_date.tzinfo is None: pub_date = pub_date.replace(tzinfo=timezone.utc)
                if pub_date >= limit:
                    all_articles.append({
                        "source": name, "title": entry.get('title', ''),
                        "link": entry.get('link', ''), "published_at": entry.get('published', ''),
                        "summary": entry.get('summary', '')[:600]
                    })
            except: continue
    
    with open('breaking_news.json', 'w', encoding='utf-8') as f:
        json.dump({"articles": all_articles}, f, ensure_ascii=False, indent=4)
    print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(all_articles)}ê±´ ì •ì œ.")

if __name__ == "__main__": collect()
