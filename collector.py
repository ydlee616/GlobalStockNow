#!/usr/bin/env python3
# GlobalStockNow News Collector v0.1 (2026.1.2)
# Google News RSS + í‚¤ì›Œë“œ í•„í„°ë§ (ë°˜ë„ì²´, Fed, Tesla ë“± 30ê°œ í‚¤ì›Œë“œ)

import feedparser
from datetime import datetime, timedelta
import json
import re
from fuzzywuzzy import fuzz  # ì¤‘ë³µ ì œê±°ìš© (pip í•„ìš” ì—†ìŒ, GitHub Actionsì— ìˆìŒ)

# PDF í˜ì´ì§€ 8 ê¸°ë°˜: 30+ ì£¼ì‹ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ìµœì´ˆ ë²„ì „)
STOCK_KEYWORDS = [
    'semiconductor', 'chip', 'nvidia', 'amd', 'intel', 'tsmc', 'samsung', 'skhynix',
    'fed', 'federal reserve', 'interest rate', 'powell',
    'tesla', 'ev', 'battery', 'byd', 'catl',
    'apple', 'iphone', 'aapl', 'googl', 'msft', 'amzn', 'meta',
    'oil', 'opec', 'energy', 'exxon',
    'china', 'trade war', 'tariff', 'hkex', 'hsi',
    'bitcoin', 'crypto', 'eth', 'sec',
    'inflation', 'cpi', 'gdp', 'recession'
]

def collect_breaking_news(max_hours=6, max_items=20):
    """Google News RSSë¡œ ìµœê·¼ 4~6ì‹œê°„ ì†ë³´ ìˆ˜ì§‘ + í‚¤ì›Œë“œ í•„í„°"""
    print("ğŸš€ GlobalStockNow ì†ë³´ ìˆ˜ì§‘ ì‹œì‘ (ìµœê·¼ {}ì‹œê°„, ìµœëŒ€ {}ê°œ)".format(max_hours, max_items))
    
    # Google News RSS (US + Business + Tech, ë¬´ë£Œ/ë¬´ì œí•œ)
    rss_feeds = [
        'https://news.google.com/rss?hl=en-US&gl=US&ceid=US:en&topic=h',  # Headlines
        'https://news.google.com/rss?hl=en-US&gl=US&ceid=US:en&cat=Bus',  # Business
        'https://news.google.com/rss?hl=en-US&gl=US&ceid=US:en&cat=Tec'   # Technology
    ]
    
    all_news = []
    seen_titles = set()  # ì¤‘ë³µ ì œê±°
    
    cutoff_time = datetime.utcnow() - timedelta(hours=max_hours)
    
    for feed_url in rss_feeds:
        feed = feedparser.parse(feed_url)
        print(f"ğŸ“¡ {feed_url} ìˆ˜ì§‘: {len(feed.entries)}ê°œ ì›ë³¸ ê¸°ì‚¬")
        
        for entry in feed.entries[:10]:  # í”¼ë“œë‹¹ ìƒìœ„ 10ê°œë§Œ
            pub_date = entry.get('published_parsed') or entry.get('updated_parsed')
            if not pub_date:
                continue
                
            pub_dt = datetime(*pub_date[:6])
            if pub_dt < cutoff_time:
                continue  # 6ì‹œê°„ ì´ë‚´ ê¸°ì‚¬ë§Œ
            
            title = entry.title.lower()
            link = entry.link
            summary = (entry.get('summary') or '').lower()
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ (ì œëª©+ìš”ì•½ 70% ì´ìƒ ì¼ì¹˜)
            content = title + ' ' + summary
            matched_keywords = [kw for kw in STOCK_KEYWORDS if kw in content]
            
            if matched_keywords and fuzz.ratio(title, list(seen_titles)[-1] if seen_titles else '') < 80:
                news_item = {
                    'title': entry.title,
                    'link': link,
                    'published': pub_dt.strftime('%Y-%m-%d %H:%M UTC'),
                    'keywords': matched_keywords[:3],  # ìƒìœ„ 3ê°œë§Œ
                    'summary': entry.get('summary', '')[:200] + '...'
                }
                all_news.append(news_item)
                seen_titles.add(title)
    
    # ì˜í–¥ë„ ë†’ì€ ìˆœ ì •ë ¬ (í‚¤ì›Œë“œ ìˆ˜ ê¸°ì¤€, ë‚˜ì¤‘ AI ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´)
    all_news.sort(key=lambda x: len(x['keywords']), reverse=True)
    final_news = all_news[:max_items]
    
    print(f"âœ… ìµœì¢… í•„í„°ë§: {len(final_news)}ê°œ ì†ë³´ ìˆ˜ì§‘ ì™„ë£Œ!")
    return final_news

if __name__ == "__main__":
    news = collect_breaking_news(max_hours=6, max_items=20)
    print("\nğŸ“Š ìˆ˜ì§‘ëœ í•´ì™¸ ì£¼ì‹ ì†ë³´ TOP 5 (í•œêµ­ ì‹œì¥ ì˜í–¥ ì˜ˆìƒ):")
    for i, item in enumerate(news[:5], 1):
        print(f"{i}. [{item['published']}] {item['title']}")
        print(f"   ğŸ”— {item['link']}")
        print(f"   ğŸ·ï¸  í‚¤ì›Œë“œ: {', '.join(item['keywords'])}")
        print()
    
    # JSON ì €ì¥ (ë‹¤ìŒ AI ë¶„ì„ ëª¨ë“ˆìš©)
    with open('breaking_news.json', 'w') as f:
        json.dump(news, f, indent=2, ensure_ascii=False)
    print("ğŸ’¾ breaking_news.json ì €ì¥ ì™„ë£Œ (AI ë¶„ì„ ì¤€ë¹„)")
