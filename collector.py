#!/usr/bin/env python3
# GlobalStockNow AI Analyzer v0.1 (2026.1.2)
# Qwen2.5-7Bë¡œ í•´ì™¸ ì†ë³´ â†’ í•œêµ­ ì‹œì¥ ì˜í–¥ ë¶„ì„

import json
from datetime import datetime
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# ë¶„ì„ ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
analyzed_news = []

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ì‚¬ì—…ê³„íšì„œ ê¸°ë°˜)
PROMPT_TEMPLATE = """
ë„ˆëŠ” í•œêµ­ ì£¼ì‹ ì „ë¬¸ê°€ì´ì ê¸€ë¡œë²Œ ê²½ì œ ë¶„ì„ê°€ë‹¤.
ì•„ë˜ í•´ì™¸ ì†ë³´ë¥¼ ë¶„ì„í•´ì„œ í•œêµ­ ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ 10ì  ë§Œì ìœ¼ë¡œ í‰ê°€í•´ì¤˜.

ë‰´ìŠ¤ ì œëª©: {title}
ë‰´ìŠ¤ ìš”ì•½: {summary}
ë°œí–‰ ì‹œê°„: {published}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•´ (ì„¤ëª… ì—†ì´ JSONë§Œ ì¶œë ¥):
{{
  "impact_score": 0~10 (ìˆ«ìë§Œ, í•œêµ­ ì‹œì¥ ì˜í–¥ë„. 7ì  ë¯¸ë§Œì€ ë¶„ì„ ì œì™¸),
  "impact_period": "ë‹¨ê¸°(1~3ì¼)" ë˜ëŠ” "ì¤‘ê¸°(1~4ì£¼)" ë˜ëŠ” "ì¥ê¸°(1ê°œì›” ì´ìƒ)",
  "related_korean_stocks": ["ì¢…ëª©ëª…1", "ì¢…ëª©ëª…2", ...] (ìµœëŒ€ 3ê°œ),
  "key_points": "í•œêµ­ íˆ¬ììì—ê²Œ ì¤‘ìš”í•œ ì‹œì‚¬ì  í•œ ë¬¸ì¥ (30ì ì´ë‚´)"
}}

ë¶„ì„ ì‹œì‘:
"""

print("ğŸ§  GlobalStockNow AI ë¶„ì„ ì‹œì‘! Qwen2.5-7B ë¡œë”© ì¤‘...")

# Qwen2.5-7B 4bit quantization (GitHub Actions GPU ì§€ì› ì—†ìŒ â†’ CPU ëª¨ë“œ, ì²« ì‹¤í–‰ 2~3ë¶„ ì†Œìš”)
model_name = "Qwen/Qwen2.5-7B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_4bit=True  # ë©”ëª¨ë¦¬ ì ˆì•½
)

print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! ë¶„ì„ ì‹œì‘")

# breaking_news.json ì½ê¸°
try:
    with open('breaking_news.json', 'r', encoding='utf-8') as f:
        news_list = json.load(f)
    print(f"ğŸ“¥ {len(news_list)}ê°œ ì†ë³´ ë¡œë“œ ì™„ë£Œ")
except:
    print("âŒ breaking_news.json íŒŒì¼ ì—†ìŒ. collector ë¨¼ì € ì‹¤í–‰ í•„ìš”")
    exit()

# í•˜ë‚˜ì”© ë¶„ì„
for idx, item in enumerate(news_list, 1):
    print(f"\n[{idx}/{len(news_list)}] ë¶„ì„ ì¤‘: {item['title'][:60]}...")
    
    prompt = PROMPT_TEMPLATE.format(
        title=item['title'],
        summary=item.get('summary', 'ìš”ì•½ ì—†ìŒ'),
        published=item['published']
    )
    
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=200,
            temperature=0.3,
            do_sample=False
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response = response[len(prompt):].strip()  # í”„ë¡¬í”„íŠ¸ ì œê±°
    
    try:
        result = json.loads(response)
        if result.get("impact_score", 0) >= 7:
            result["original_title"] = item["title"]
            result["original_link"] = item["link"]
            result["analyzed_at"] = datetime.utcnow().strftime('%Y-%m-%d %H:%M UTC')
            analyzed_news.append(result)
            print(f"   âœ… ì˜í–¥ë„ {result['impact_score']}ì  â†’ ë¶„ì„ ì €ì¥")
        else:
            print(f"   â© ì˜í–¥ë„ {result.get('impact_score', 0)}ì  â†’ ìŠ¤í‚µ")
    except:
        print("   âŒ JSON íŒŒì‹± ì‹¤íŒ¨ â†’ ìŠ¤í‚µ")

# ê²°ê³¼ ì €ì¥
final_count = len(analyzed_news)
print(f"\nğŸ¯ ìµœì¢… ë¶„ì„ ì™„ë£Œ: {final_count}ê°œ ê³ ì˜í–¥ ì†ë³´ ì„ ë³„!")

with open('analyzed_news.json', 'w', encoding='utf-8') as f:
    json.dump(analyzed_news, f, indent=2, ensure_ascii=False)

print("ğŸ’¾ analyzed_news.json ì €ì¥ ì™„ë£Œ â†’ ì½˜í…ì¸  ìƒì„± ì¤€ë¹„ OK!")
